{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Encoder Decoder\n",
    "Inspired by https://arxiv.org/abs/1802.01817"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display_html\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "params = tf.contrib.training.HParams(\n",
    "    max_string_length = 64,\n",
    "    model_n = 8,\n",
    "    \n",
    "    batch_size = 16,\n",
    "    shuffle_buffer = 10000,\n",
    "    num_epochs = 10,\n",
    "    \n",
    "    conv_n_filters = 256,\n",
    "    conv_filter_size = 3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a string of text into a vector of char codes.\n",
    "def _text_to_codes(text_string):\n",
    "    text_string = text_string.strip()[:params.max_string_length]\n",
    "    in_array = np.array([ord(c) for c in text_string], dtype=np.uint8)\n",
    "    return in_array\n",
    "\n",
    "def byte_hot_encoding(byte_codes):\n",
    "    return tf.one_hot(byte_codes, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a prediction back to a string.\n",
    "def _codes_to_string(codes):\n",
    "    return \"\".join((chr(c) for c in codes))\n",
    "\n",
    "def _argmax_to_string(argmaxes):\n",
    "    return \"\".join((chr(np.argmax(c)) for c in argmaxes))\n",
    "\n",
    "def decode_text(decoded_tensor):\n",
    "    from_max = tf.argmax(decoded_tensor, axis=-1)\n",
    "    return from_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_transform_to_internal(input_data, layer_id):\n",
    "    with tf.variable_scope(\"input_transform_%d\" % layer_id):\n",
    "      layer_outputs = [input_data]\n",
    "      for _ in range(params.model_n):\n",
    "        layer_output = tf.layers.conv1d(layer_outputs[-1],filters=params.conv_n_filters, kernel_size=params.conv_filter_size, padding=\"same\", activation=tf.nn.relu)\n",
    "        if len(layer_outputs) > 2:\n",
    "          # add residual connection\n",
    "          layer_output += layer_outputs[-2]\n",
    "        layer_outputs.append(layer_output)\n",
    "    return layer_outputs[-1]\n",
    "    \n",
    "def encoder_recursion_layer(input_data, layer_id):\n",
    "    processed = encoder_transform_to_internal(input_data, layer_id)\n",
    "    return tf.layers.max_pooling1d(processed, pool_size=2, strides = 2)\n",
    "\n",
    "def encoder_fully_connected(input_data):\n",
    "    with tf.variable_scope(\"encoder_fully_connected\"):\n",
    "        flattened = tf.contrib.layers.flatten(input_data)\n",
    "        layer_size = flattened.get_shape()[-1].value\n",
    "        layers_output = [flattened]\n",
    "        for _ in range(params.model_n):\n",
    "            layer_output = tf.contrib.layers.fully_connected(layers_output[-1], layer_size)\n",
    "            if len(layers_output) > 2:\n",
    "                layer_output += layers_output[-2]\n",
    "            layers_output.append(layer_output)\n",
    "    return layers_output[-1]\n",
    "    \n",
    "\n",
    "def build_encoder(input_data):\n",
    "    data = [encoder_transform_to_internal(input_data, 0)]\n",
    "    for i in range(1, 5):\n",
    "      data.append(encoder_recursion_layer(data[-1], i))\n",
    "    # return encoder_fully_connected(data[-1])    \n",
    "    return data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_fully_connected(input_data):\n",
    "    with tf.variable_scope(\"decoder_fully_connected\"):\n",
    "        layer_size = input_data.get_shape()[-1].value\n",
    "        layer_outputs = [input_data]\n",
    "        for _ in range(params.model_n):\n",
    "            layer_output = tf.contrib.layers.fully_connected(layer_outputs[-1], layer_size)\n",
    "            if len(layer_outputs) > 2:\n",
    "                layer_output += layer_outputs[-2]\n",
    "            layer_outputs.append(layer_output)\n",
    "        # Unflatten data.\n",
    "        output = tf.reshape(layer_outputs[-1], shape=[-1,layer_size/params.conv_n_filters, params.conv_n_filters])\n",
    "    return output\n",
    "\n",
    "def decoder_transform_to_external(input_data, depth, layer_id):\n",
    "    with tf.variable_scope(\"decoder_transform_%d\" % layer_id):\n",
    "      layer_outputs = [input_data]\n",
    "      for _ in range(depth):\n",
    "        layer_output = tf.layers.conv1d(layer_outputs[-1],filters=params.conv_n_filters, kernel_size=params.conv_filter_size, padding=\"same\", activation=tf.nn.relu)\n",
    "        # Extend size \n",
    "        if len(layer_outputs) > 2:\n",
    "          # add residual connection\n",
    "          layer_output += layer_outputs[-2]\n",
    "        layer_outputs.append(layer_output)\n",
    "    return layer_outputs[-1]\n",
    "\n",
    "def decoder_recursion_layer(input_data, layer_id):\n",
    "    with tf.variable_scope(\"decoder_expansion_%d\" % layer_id):\n",
    "        processed = decoder_transform_to_external(input_data, params.model_n-1,layer_id)\n",
    "        # Expand convolution.\n",
    "        expanded = tf.layers.conv1d(processed,filters=2*params.conv_n_filters, kernel_size=params.conv_filter_size, padding=\"same\", activation=tf.nn.relu)\n",
    "        # Un-sampling is done by resize.\n",
    "        expanded_shape = expanded.get_shape()\n",
    "        expanded = tf.reshape(expanded, shape = [-1,expanded_shape[1].value*2,expanded_shape[2].value/2])\n",
    "        return expanded\n",
    "\n",
    "def build_decoder(output_data):\n",
    "    #data = [decoder_fully_connected(output_data)]\n",
    "    data = [output_data]\n",
    "    for i in range(5,1, -1):\n",
    "        data.append(decoder_recursion_layer(data[-1], i))\n",
    "    return decoder_transform_to_external(data[-1], params.model_n, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Loss 0.693707764149, step 429601 <BR><table>\n",
       "                <tr><th>Source</th><th>Decoded</th></tr><tr><td>So, if there's a gene causing this trimming  and if that gene mu</td><td>So, if there's a more barling this prestin,  and is that were su</td></tr><tr><td>And this person was very forceful and said,  \"Look, that's the o</td><td>And this porple was very pentenes and said,  \"npene that's the o</td></tr><tr><td>Very impressive.\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000</td><td>He's inpreumive.\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000</td></tr><tr><td>This dead zone is on the order of tens of microns thick,  so tha</td><td>This be'd lime is on the adeed of port of hister, shank,  so the</td></tr><tr><td>Not only that,  but I lost touch of where my food came from.\u0000\u0000\u0000\u0000</td><td>Now onto that I bar a most years of these mo lord same proce.\u0000\u0000\u0000</td></tr><tr><td>Because we really need to make significant change.\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000</td><td>Because we realle need to make dathemoring change.\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000</td></tr><tr><td>I proposed this stupid story to National Geographic.\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000</td><td>I protuted this po ses stare to faternal contiratic.\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000</td></tr><tr><td>Many people feel uncomfortable with the idea of wading  into Flo</td><td>Many people been intereattably with the odea of hanten  on,a so </td></tr><tr><td>We just took this character that I just talked about,  put it on</td><td>We jast tord this sreatine  what I kusy falled anou, I but it an</td></tr><tr><td>If he's using path integration,  then it should end up in the wr</td><td>If teos as to mach intermation,  thes is proa,  and st in the pr</td></tr><tr><td>When that mother and infant lock eyes,  and the infant's old eno</td><td>When that litere and ansi a tord anar,  ane the antere,  and ofe</td></tr><tr><td>My family ate one meal per day, at night.\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000</td><td>My simelr an, and leod hir kay, ut lonht.\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000</td></tr><tr><td>They were followed by generation two.\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000</td><td>They were balline  by peneration wwo.\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000</td></tr><tr><td>Our time-lapse units are in Alaska, the Rockies, Greenland and I</td><td>Our somerilite are,s are in clanes, the contiite precienes and t</td></tr><tr><td>Okay let me do this differently. Is anybody preferring the Wonde</td><td>Onen let me do this differente,  at anately profeching the conte</td></tr><tr><td>From the youngest of ages, we display  a prodigious sporting tal</td><td>From the countiot of all,, we dippenes a procectors sreate   for</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 31.006\n",
      "INFO:tensorflow:Saving checkpoints for 429621 into ./checkpoint/model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    dataset = tf.contrib.data.TextLineDataset(\"train.en\")\n",
    "    dataset = dataset.repeat(params.num_epochs)\n",
    "    dataset = dataset.shuffle(params.shuffle_buffer)\n",
    "    dataset = dataset.map(lambda text_string: tf.py_func(_text_to_codes,[text_string], tf.uint8))\n",
    "    dataset = dataset.padded_batch(params.batch_size, [params.max_string_length])\n",
    "    dataset = dataset.map(byte_hot_encoding)\n",
    "    training_iterator = dataset.make_initializable_iterator()\n",
    "    next_element = training_iterator.get_next()\n",
    "    encoded = build_encoder(next_element)\n",
    "    predicted = build_decoder(encoded)\n",
    "    predicted_text = decode_text(predicted)\n",
    "    loss = tf.losses.softmax_cross_entropy(tf.reshape(next_element, shape=[-1,256]), tf.reshape(predicted, shape=[-1, 256]))\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    exponential_decay_fn = functools.partial(\n",
    "      tf.train.exponential_decay,\n",
    "      decay_steps=1000,\n",
    "      decay_rate=0.5,\n",
    "      staircase=True)\n",
    "    training_op = tf.contrib.layers.optimize_loss(\n",
    "        loss,\n",
    "        global_step,\n",
    "        learning_rate=None,\n",
    "        optimizer=optimizer,\n",
    "        learning_rate_decay_fn=exponential_decay_fn)\n",
    "    scaffold = tf.train.Scaffold(\n",
    "        local_init_op=tf.group(tf.local_variables_initializer(),training_iterator.initializer),\n",
    "        init_op=tf.global_variables_initializer())\n",
    "    with tf.train.MonitoredTrainingSession(checkpoint_dir=\"./checkpoint\",scaffold=scaffold, save_checkpoint_secs=120, save_summaries_secs=60) as sess:\n",
    "        print \"Start training\"\n",
    "        while not sess.should_stop():\n",
    "            o_loss, _, o_step = sess.run([loss, training_op, global_step])\n",
    "            if o_step%100 == 1:\n",
    "                clear_output(True)\n",
    "                output_string = \"\"\"Loss {0}, step {1} <BR><table>\n",
    "                <tr><th>Source</th><th>Decoded</th></tr>\"\"\".format(o_loss, o_step)\n",
    "                o_source_text, o_predicted_text = sess.run([next_element, predicted_text])\n",
    "                for t_source, t_predicted in zip((x for x in o_source_text),(y for y in o_predicted_text)):\n",
    "                    output_string += \"\"\"<tr><td>{source}</td><td>{decoded}</td></tr>\"\"\".format(source=_argmax_to_string(t_source),\n",
    "                                                                                               decoded=_codes_to_string(t_predicted))\n",
    "                output_string += \"</table>\"\n",
    "                display_html(output_string, raw=True)\n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
