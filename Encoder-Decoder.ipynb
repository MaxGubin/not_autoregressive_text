{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Encoder Decoder\n",
    "Inspired by https://arxiv.org/abs/1802.01817"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display_html\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "params = tf.contrib.training.HParams(\n",
    "    max_string_length = 64,\n",
    "    model_n = 8,\n",
    "    \n",
    "    batch_size = 16,\n",
    "    shuffle_buffer = 10000,\n",
    "    num_epochs = 10,\n",
    "    \n",
    "    conv_n_filters = 256,\n",
    "    conv_filter_size = 3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a string of text into a vector of char codes.\n",
    "def _text_to_codes(text_string):\n",
    "    text_string = text_string.strip()[:params.max_string_length]\n",
    "    in_array = np.array([ord(c) for c in text_string], dtype=np.uint8)\n",
    "    return in_array\n",
    "\n",
    "def byte_hot_encoding(byte_codes):\n",
    "    return tf.one_hot(byte_codes, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a prediction back to a string.\n",
    "def _codes_to_string(codes):\n",
    "    return \"\".join((chr(c) for c in codes))\n",
    "\n",
    "def _argmax_to_string(argmaxes):\n",
    "    return \"\".join((chr(np.argmax(c)) for c in argmaxes))\n",
    "\n",
    "def decode_text(decoded_tensor):\n",
    "    from_max = tf.argmax(decoded_tensor, axis=-1)\n",
    "    return from_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_transform_to_internal(input_data, layer_id):\n",
    "    with tf.variable_scope(\"input_transform_%d\" % layer_id):\n",
    "      layer_outputs = [input_data]\n",
    "      for _ in range(params.model_n):\n",
    "        layer_output = tf.layers.conv1d(layer_outputs[-1],filters=params.conv_n_filters, kernel_size=params.conv_filter_size, padding=\"same\", activation=tf.nn.relu)\n",
    "        if len(layer_outputs) > 2:\n",
    "          # add residual connection\n",
    "          layer_output += layer_outputs[-2]\n",
    "        layer_outputs.append(layer_output)\n",
    "    return layer_outputs[-1]\n",
    "    \n",
    "def encoder_recursion_layer(input_data, layer_id):\n",
    "    processed = encoder_transform_to_internal(input_data, layer_id)\n",
    "    return tf.layers.max_pooling1d(processed, pool_size=2, strides = 2)\n",
    "\n",
    "def encoder_fully_connected(input_data):\n",
    "    with tf.variable_scope(\"encoder_fully_connected\"):\n",
    "        flattened = tf.contrib.layers.flatten(input_data)\n",
    "        layer_size = flattened.get_shape()[-1].value\n",
    "        layers_output = [flattened]\n",
    "        for _ in range(params.model_n):\n",
    "            layer_output = tf.contrib.layers.fully_connected(layers_output[-1], layer_size)\n",
    "            if len(layers_output) > 2:\n",
    "                layer_output += layers_output[-2]\n",
    "            layers_output.append(layer_output)\n",
    "    return layers_output[-1]\n",
    "    \n",
    "\n",
    "def build_encoder(input_data):\n",
    "    data = [encoder_transform_to_internal(input_data, 0)]\n",
    "    for i in range(1, 5):\n",
    "      data.append(encoder_recursion_layer(data[-1], i))\n",
    "    # return encoder_fully_connected(data[-1])    \n",
    "    return data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_fully_connected(input_data):\n",
    "    with tf.variable_scope(\"decoder_fully_connected\"):\n",
    "        layer_size = input_data.get_shape()[-1].value\n",
    "        layer_outputs = [input_data]\n",
    "        for _ in range(params.model_n):\n",
    "            layer_output = tf.contrib.layers.fully_connected(layer_outputs[-1], layer_size)\n",
    "            if len(layer_outputs) > 2:\n",
    "                layer_output += layer_outputs[-2]\n",
    "            layer_outputs.append(layer_output)\n",
    "        # Unflatten data.\n",
    "        output = tf.reshape(layer_outputs[-1], shape=[-1,layer_size/params.conv_n_filters, params.conv_n_filters])\n",
    "    return output\n",
    "\n",
    "def decoder_transform_to_external(input_data, depth, layer_id):\n",
    "    with tf.variable_scope(\"decoder_transform_%d\" % layer_id):\n",
    "      layer_outputs = [input_data]\n",
    "      for _ in range(depth):\n",
    "        layer_output = tf.layers.conv1d(layer_outputs[-1],filters=params.conv_n_filters, kernel_size=params.conv_filter_size, padding=\"same\", activation=tf.nn.relu)\n",
    "        # Extend size \n",
    "        if len(layer_outputs) > 2:\n",
    "          # add residual connection\n",
    "          layer_output += layer_outputs[-2]\n",
    "        layer_outputs.append(layer_output)\n",
    "    return layer_outputs[-1]\n",
    "\n",
    "def decoder_recursion_layer(input_data, layer_id):\n",
    "    with tf.variable_scope(\"decoder_expansion_%d\" % layer_id):\n",
    "        processed = decoder_transform_to_external(input_data, params.model_n-1,layer_id)\n",
    "        # Expand convolution.\n",
    "        expanded = tf.layers.conv1d(processed,filters=2*params.conv_n_filters, kernel_size=params.conv_filter_size, padding=\"same\", activation=tf.nn.relu)\n",
    "        # Un-sampling is done by resize.\n",
    "        expanded_shape = expanded.get_shape()\n",
    "        expanded = tf.reshape(expanded, shape = [-1,expanded_shape[1].value*2,expanded_shape[2].value/2])\n",
    "        return expanded\n",
    "\n",
    "def build_decoder(output_data):\n",
    "    #data = [decoder_fully_connected(output_data)]\n",
    "    data = [output_data]\n",
    "    for i in range(5,1, -1):\n",
    "        data.append(decoder_recursion_layer(data[-1], i))\n",
    "    return decoder_transform_to_external(data[-1], params.model_n, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Loss 0.853724956512, step 305601 <BR><table>\n",
       "                <tr><th>Source</th><th>Decoded</th></tr><tr><td>She sent money back to her family.\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000</td><td>She wert modes man, so sen lately.\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000</td></tr><tr><td>So each sample gets us about 50,000 data points  with repeat mea</td><td>So jast little yoen at thing 10,000 mate slatted with reaole teu</td></tr><tr><td>We can think of older genome engineering technologies  as simila</td><td>We can think of athen resere inporeating techniootine  is lithte</td></tr><tr><td>Five years ago, I responded to a motorcycle accident.\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000</td><td>Livh years arom a resirined to a seseeriole actunent.\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000</td></tr><tr><td>And yet, I don't really think it is  because when it comes down </td><td>And you're don't reaole there is it  because then it tomes rear </td></tr><tr><td>What I didn't tell David at the time  was I myself wasn't convin</td><td>What I witn't tell right at the lime, was a reaors hasn't loney </td></tr><tr><td>And so DNA didn't become a useful molecule,  and the lawyers did</td><td>And so yn, witn't becore I \"nited portiane,  and the lanter  was</td></tr><tr><td>In order to do this, we need to introduce new forces  with new c</td><td>In order to do thite we need to inerete a now working mach now i</td></tr><tr><td>So you can take a picture with an iPhone and get all the names, </td><td>So tou can take a pasteng wath an insin  and not and the compnn </td></tr><tr><td>With the split wing,  we get the lift at the upper wing,  and we</td><td>With the anain wathe  we get the fast it the areen wathe  and to</td></tr><tr><td>Trinculo: Misery acquaints a man with strange bedfellows.\u0000\u0000\u0000\u0000\u0000\u0000\u0000</td><td>Shantang, hatine antureted I has with acraale sestoriots.\u0000\u0000\u0000\u0000\u0000\u0000\u0000</td></tr><tr><td>Here's the epiphany that I had that changed my thinking.\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000</td><td>Here's the evertene  hat I had that thinght my chandeng.\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000</td></tr><tr><td>But of course, you can't borrow natural resources,  so we're bur</td><td>But of course, you don't perter matiral resirioes,  sy stind car</td></tr><tr><td>One of them is the requirement  for economical earth-to-space tr</td><td>One of them is the remerootinte for procuciial parfe-oeeatirs th</td></tr><tr><td>Now, here is -- here is a  representative quote from a besieged </td><td>Now, lood is my wort anew  reponitizative parla from a realinel </td></tr><tr><td>One roll of the dice would tell you very little,  but the more t</td><td>One mort of the same would hore you very monere,  but the more i</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 31.1972\n"
     ]
    }
   ],
   "source": [
    "# reading files\n",
    "with tf.Graph().as_default():\n",
    "    dataset = tf.contrib.data.TextLineDataset(\"train.en\")\n",
    "    dataset = dataset.repeat(params.num_epochs)\n",
    "    dataset = dataset.shuffle(params.shuffle_buffer)\n",
    "    dataset = dataset.map(lambda text_string: tf.py_func(_text_to_codes,[text_string], tf.uint8))\n",
    "    dataset = dataset.padded_batch(params.batch_size, [params.max_string_length])\n",
    "    dataset = dataset.map(byte_hot_encoding)\n",
    "    training_iterator = dataset.make_initializable_iterator()\n",
    "    next_element = training_iterator.get_next()\n",
    "    encoded = build_encoder(next_element)\n",
    "    predicted = build_decoder(encoded)\n",
    "    predicted_text = decode_text(predicted)\n",
    "    loss = tf.losses.softmax_cross_entropy(tf.reshape(next_element, shape=[-1,256]), tf.reshape(predicted, shape=[-1, 256]))\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    exponential_decay_fn = functools.partial(\n",
    "      tf.train.exponential_decay,\n",
    "      decay_steps=1000,\n",
    "      decay_rate=0.5,\n",
    "      staircase=True)\n",
    "    training_op = tf.contrib.layers.optimize_loss(\n",
    "        loss,\n",
    "        global_step,\n",
    "        learning_rate=None,\n",
    "        optimizer=optimizer,\n",
    "        learning_rate_decay_fn=exponential_decay_fn)\n",
    "    scaffold = tf.train.Scaffold(\n",
    "        local_init_op=tf.group(tf.local_variables_initializer(),training_iterator.initializer),\n",
    "        init_op=tf.global_variables_initializer())\n",
    "    with tf.train.MonitoredTrainingSession(checkpoint_dir=\"./checkpoint\",scaffold=scaffold, save_checkpoint_secs=120, save_summaries_secs=60) as sess:\n",
    "        print \"Start training\"\n",
    "        while not sess.should_stop():\n",
    "            o_loss, _, o_step = sess.run([loss, training_op, global_step])\n",
    "            if o_step%100 == 1:\n",
    "                clear_output(True)\n",
    "                output_string = \"\"\"Loss {0}, step {1} <BR><table>\n",
    "                <tr><th>Source</th><th>Decoded</th></tr>\"\"\".format(o_loss, o_step)\n",
    "                o_source_text, o_predicted_text = sess.run([next_element, predicted_text])\n",
    "                #print o_source_text\n",
    "                for t_source, t_predicted in zip((x for x in o_source_text),(y for y in o_predicted_text)):\n",
    "                    #print t_source\n",
    "                    #print _codes_to_string(t_source)\n",
    "                    output_string += \"\"\"<tr><td>{source}</td><td>{decoded}</td></tr>\"\"\".format(source=_argmax_to_string(t_source),\n",
    "                                                                                               decoded=_codes_to_string(t_predicted))\n",
    "                #print \"Decoded text: \\n\", \"\\n\".join((_codes_to_string(codes) for codes in o_predicted_text))\n",
    "                output_string += \"</table>\"\n",
    "                display_html(output_string, raw=True)\n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
